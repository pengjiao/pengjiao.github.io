<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Pengjiao`s blog</title>
    <link>/statistics/</link>
    <description>Recent content in Statistics on Pengjiao`s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Sep 2020 14:01:19 +0800</lastBuildDate>
    
	<atom:link href="/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PCA和PLS基本原理</title>
      <link>/statistics/pca_plsda/</link>
      <pubDate>Tue, 08 Sep 2020 15:45:45 +0800</pubDate>
      
      <guid>/statistics/pca_plsda/</guid>
      <description>这篇文章主要总结了PCA和PLS的基本原理，用于更好的理解实际应用，不涉及太多数学推导。
  PCA基本原理
  PLS基本原理
   PCA基本原理 主成分分析（Principal component analysis, PCA）是一种非监督学习方法，在保留原有数据最大方差的同时，通过识别主成分（principle components）将高维数据降至低维。
虽然经常用PCA进行数据的降维和样本的聚类，但是对PCA如何实现这一过程不够了解。所以通过学习StatQuest和Process Improvement Using Data，总结了一下PCA对基本原理。
现在有一个矩阵X，包括了N行和K列。其中N行是观测值（如不同基因的表达值），K列是变量（如不同的样本）。接下来以3列（3维）为例：
  首先PCA通过centering和scaling，将原始数据移至坐标系的中心，并且每个变量有相等的缩放。   然后寻找一条直线能最佳的拟合数据（best-fit）。最佳拟合直线的判断标准为，各观测点到最佳拟合直线的残差（residual error）最小。残差是指观测点到最佳拟合直线的垂直距离，PCA中称为得分（score）。这一个衡量方式等同于最佳拟合直线沿着观测点投影到该线到最大方差方向,所有观测点沿最佳拟合直线的距离的平方和称为平方和距离（sum of squared distances，SSD）。   这条最佳拟合直线即为PCA中的主成分1（principle component 1, PCA），同时也被称为第一潜在变量（first latent variable）。在PC1上一个单位长的向量被称为奇异向量（singular vector）或特征向量（eigenvector）,其中每一个变量在其中的比例成为负载得分（loading score）。PC1上的平方和距离SSD称为PC1的特征值(eigenvalue)。PC1特征值的平方根称为PC1的奇异值（singular value）。
  确定PCA1之后，PC2即为经过原点的与PC1垂直的直线。
  然后将PC1和PC2转动至水平位置，根据PC1和PC2上的投影点确定观测点的位置，这就是通过奇异值分解（ singular value decomposition, SVD）计算PCA的过程。
  </description>
    </item>
    
  </channel>
</rss>